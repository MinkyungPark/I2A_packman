# I2A Architecture
- keywords : data efficiency, Imperfection model(e.g. f-approx) robustness, policy distillation
- Training $\pi, V$ : policy gradient, value function approximation

<br>

![](https://t1.daumcdn.net/cfile/tistory/99BF2C3E5B0CFFC626)

<br>

a) Imagination core <br>
- $\hat{\pi}$ Simulation ot -> ot+1, rt+1, ot+2, rt+2...
- rollout strategy : Policy distillation
  - pretrained policy(imperfect model) : $\hat{\pi}(o_t)$
  - imagination-augmented policy : $\pi(o_t)$
  - $L_{dist}(\pi, \hat{\pi}) = Î»_{dist}Î£_aÏ€(a|o)log(\hat{\pi}(a|o))$
- Policy Network
  - ëœë¤, pretrained model ... ë“±ë“± ì—¬ê¸°ì„œëŠ” a2cë¥¼ ì‚¬ìš©
  - out, nê°œì˜ trajectories  $\hat{Ï„}_1,...\hat{Ï„}_n$
- Environment Model
  - distribution which is optimized by using NLL($l_{model} target Reward,Stateì™€ imagined Reward,stateì™€ì˜ negative likelihood loss$)
  - í™˜ê²½ ëª¨ë¸ì˜ dynamicì´ë‘ env_modelì˜ dynamicsë‘ $l_{model}$ì„ í†µí•´ ë§ì¶˜ë‹¤.
  - Learning Env dynamics from training data from partial trained model-free agent(Policy Network) trajectories(~$\hat{\pi}$)

b) Encode <br>
- â­ï¸ Rollout encoding features is out encoded possible trajectories
    - rollout embedding $e_i \ = \ Ïµ(\hat{Î¤}_i)$

    - Aggregator out singloe imagination code $c_{ia}$ = ğ›¢$(e_1, e_2, .... e_n)$

- Imagination-augmented RL : Learn to Interpret <br>
  - model free decisionì„ ë” íš¨ìœ¨ì ìœ¼ë¡œ í•˜ê¸° ìœ„í•´ action ì „, simulationì„ í•œë‹¤.
  - immediate rewardì™€ëŠ” ìƒê´€ ì—†ì–´ë„ $Ï„$ í™•ì¸ì‹œ valueì™€ ê´€ë ¨ ë†’ì€ ì •ë³´ë¥¼ ì¶”ì¶œí•˜ê±°ë‚˜ í•„ìš” ì—†ëŠ” $Ï„$ ë¬´ì‹œ í•œë‹¤!
  - ì¦‰, imperfection modelì˜ useful but imperfect simulation knowledgeë¥¼ ì¶”ì¶œí•´ end-to-endë¡œ í•™ìŠµí•¨ <br>

c) Out <br>
- whole rollout encoding $c_ia$ + model-free path $c_mf$ => $\pi, V$

<br>

---
## Details
### a) Imagination Core(IC)
### 1. Policy Network A2C
a2c.py a2c_train.ipynb
- it can reuse Model-free path
- out, nê°œì˜ trajectories $\hat{Ï„}_1, ...\hat{Ï„}_n$

### 2. Env Model
env_model.py env_model_train.ipynb
<br>

<a href='https://ifh.cc/v-69TTc2' target='_blank'><img src='https://ifh.cc/g/69TTc2.png' border='0'></a>
- Env Model training : predict next frame and reward by stochastic gradient descent on the bernoulli cross-entropy(negative log-likelihood loss $l_{model}$)
- Reward Vector $w_{rew} \in R^5$ = moving/ eating food/ eating a power pill/ eating a ghost/ being eaten by a ghost
  - reward is sparse
  - prediction of ghost dynamics is important

### b) Imagination Rollout Encoder
encoder.py

<br>

---

<br>

### Env : MiniPacman
- state space : (3, 15, 19) -> (RGB X WIDTH X HEIGHT)
- actions : Discrete(5).  [up, left, right, down, stay]
- observation_space : spaces.Box(low=0, high=1.0, shape=(3, 15, 19))
- modes : regular, avoid, hunt, ambush, rush
- instance : MiniPacman(mode, frame_cap)

| Environment | Regular | Avoid | Hunt | Ambush | Rush |
| :----------- | :-----------: | :-----------: | :-----------------: | :-----------------: | :----------: |
| Step Reward       | 0 | 0.1 | 0 | 0 | 0 |
| Food Reward       | 1 | -0.1 | 0 | -0.1 | -0.1 |
| Power Pill Reward | 2 | -5 | 1 | 0 | -10 |
| Kill Ghost Reward | 5 | -10 | 10 | 10 | 0 |
| Death Reward      | 0 | -20 | -20 | -20 | 0 |
| Next level if all power pills eaten | No | No | No | No | Yes |
| Next level if all ghosts killed | No | No | Yes | Yes | No |
| Next level if all food eaten | Yes | Yes | No | No | No |
| Next level when surviving n timesteps | Never | 128 | 80 | 80 | Never |

<br>

references
- https://github.com/mpSchrader/gym-sokoban/issues/11
- [TEMPORAL DIFFERENCE MODELS: MODEL-FREE DEEP RL FOR MODEL-BASED CONTROL](https://arxiv.org/pdf/1802.09081.pdf)
- [Learning model-based planning from scratch](https://arxiv.org/pdf/1707.06170.pdf)

