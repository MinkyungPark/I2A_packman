{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('./tensorboard/ENV')\n",
    "\n",
    "from minipackman import multi_env\n",
    "from env_model import EnvModel, ModelDyna\n",
    "from a2c import ActorCritic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_envs = 8\n",
    "mode = 'regular'\n",
    "envs = multi_env(num_envs=num_envs, mode=mode)\n",
    "state_shape = envs.observation_space.shape\n",
    "num_actions = envs.action_space.n\n",
    "\n",
    "md = ModelDyna()\n",
    "env_model = EnvModel(envs.observation_space.shape, md.num_pixels, len(md.mode_rewards[\"regular\"]))\n",
    "actor_critic = ActorCritic(envs.observation_space.shape, envs.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained(imperfect) a2c model\n",
    "actor_critic.load_state_dict(torch.load('./model/a2c_regular_150000', map_location=torch.device('cpu')))\n",
    "\n",
    "# policy hat_pi \n",
    "def get_action(state):\n",
    "    if state.ndim == 4:\n",
    "        state = torch.FloatTensor(np.float32(state))\n",
    "    else:\n",
    "        state = torch.FloatTensor(np.float32(state)).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      action = actor_critic.act(state)\n",
    "    action = action.data.cpu().squeeze(1).numpy()\n",
    "    return action\n",
    "\n",
    "\n",
    "def play_games(envs, frames):\n",
    "    states = envs.reset()\n",
    "    \n",
    "    for frame_idx in range(frames):\n",
    "        actions = get_action(states)\n",
    "        next_states, rewards, dones, _ = envs.step(actions)\n",
    "        \n",
    "        yield frame_idx, states, actions, rewards, next_states, dones\n",
    "        \n",
    "        states = next_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "reward_coef = 0.1\n",
    "num_updates = 5000\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(env_model.parameters())\n",
    "\n",
    "for frame_idx, states, actions, rewards, next_states, dones in play_games(envs, num_updates):\n",
    "    states = torch.FloatTensor(states)\n",
    "    actions = torch.LongTensor(actions)\n",
    "\n",
    "    batch_size = states.size(0)\n",
    "    \n",
    "    onehot_actions = torch.zeros(batch_size, num_actions, *state_shape[1:])\n",
    "    onehot_actions[range(batch_size), actions] = 1\n",
    "    inputs = torch.cat([states, onehot_actions], 1)\n",
    "    \n",
    "    if torch.cuda.is_available(): inputs = inputs.cuda()\n",
    "\n",
    "    imagined_state, imagined_reward = env_model(inputs)\n",
    "\n",
    "    target_state = md.pix_to_target(next_states) # from model's dynamic\n",
    "    target_state = torch.LongTensor(target_state)\n",
    "    \n",
    "    target_reward = md.rewards_to_target(mode, rewards)\n",
    "    target_reward = torch.LongTensor(target_reward)\n",
    "\n",
    "    # l_model(auxilary loss) : model's dynamic ~ our env model's dynamic\n",
    "    optimizer.zero_grad()\n",
    "    image_loss = criterion(imagined_state, target_state)\n",
    "    reward_loss = criterion(imagined_reward, target_reward)\n",
    "    loss = image_loss + reward_coef * reward_loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # log\n",
    "    writer.add_scalar('training reward', rewards.sum(), frame_idx)\n",
    "    writer.add_scalar('training loss', loss.item(), frame_idx)\n",
    "    if frame_idx % 1000 == 0 or frame_idx == num_updates - 1:\n",
    "        print(f'frame_idx : {frame_idx} :::: rewards: {rewards.sum()} :::: losses: {loss.item()}')\n",
    "        torch.save(env_model.state_dict(), './model/env_model_' + mode + '_'+ str(frame_idx+1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
